{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "The current set of notebooks are under constant development.\n",
    "\n",
    "## Update Tutorial Repository\n",
    "\n",
    "If you have previously cloned the tutorial repository, you may need to get the latest versions of the notebooks.\n",
    "\n",
    "First check the status of your repository:\n",
    "```\n",
    "cd hls4ml-tutorial\n",
    "git status \n",
    "```\n",
    "\n",
    "You may have some _modified_ notebooks. For example:\n",
    "\n",
    "```\n",
    "# On branch csee-e6868-spring2021\n",
    "# Changes not staged for commit:\n",
    "#   (use \"git add <file>...\" to update what will be committed)\n",
    "#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
    "#\n",
    "#\tmodified:   part1_getting_started.ipynb\n",
    "#\tmodified:   part2_advanced_config.ipynb\n",
    "#\n",
    "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
    "```\n",
    "\n",
    "You can make a copy of those modified notebooks if you had significat changes, otherwise the easiest thing to do is to discard those changes.\n",
    "\n",
    "**ATTENTION** You will loose your local changes!\n",
    "\n",
    "```\n",
    "git checkout *.ipynb\n",
    "```\n",
    "\n",
    "At this point, you can update you copy of the repository:\n",
    "```\n",
    "git pull\n",
    "```\n",
    "\n",
    "# Part 3: Compression\n",
    "\n",
    "The terms _sparsity_, _pruning_, and _compression_ of a neural network are correlated and synonyms in this notebook. For more details look at this blog posts [[1]](https://numenta.com/blog/2019/08/30/case-for-sparsity-in-neural-networks-part-1-pruning) and [[2]](https://numenta.com/blog/2020/10/30/case-for-sparsity-in-neural-networks-part-2-dynamic-sparsity).\n",
    "\n",
    "A sparse neural network is network where many of the weights are `0` and pruning is a compression technique to reduce the size of the neural network by _introducing more 0s_.\n",
    "\n",
    "<img src=\"images/pruning.png\" style=\"width:640px;\" />\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "As we did in the previous notebooks, let's import the libraries, call the magic functions, and setup the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/xilinx/Vivado/2019.1/bin:' + os.environ['PATH']\n",
    "def is_tool(name):\n",
    "    from distutils.spawn import find_executable\n",
    "    return find_executable(name) is not None\n",
    "\n",
    "print('-----------------------------------')\n",
    "if not is_tool('vivado_hls'):\n",
    "    print('Xilinx Vivado HLS is NOT in the PATH')\n",
    "else:\n",
    "    print('Xilinx Vivado HLS is in the PATH')\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML\n",
    "\n",
    "The [jet tagging dataset](https://www.openml.org/d/42468) is publicly available on [OpenML](https://www.openml.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now construct a model\n",
    "\n",
    "We'll use the same architecture as in the previous notebooks: 3 hidden layers with 64, then 32, then 32 neurons. Each layer will use `relu` activation. Add an output layer with 5 neurons (one for each class), then finish with Softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(16,), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu1'))\n",
    "model.add(Dense(32, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu2'))\n",
    "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='relu', name='relu3'))\n",
    "model.add(Dense(5, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to compress a model?\n",
    "\n",
    "Either you are running the model in software, as an FPGA firmware, or implemented as ASIC, a **smaller** model that retains the accuracy is always better. In some cases, you may be also willing to trade-off in accuracy for a smaller footprint, as we already have seen with _post-training quantization_.\n",
    "\n",
    "<img src=\"images/before_compression.png\" style=\"width:640px;\" />\n",
    "\n",
    "There is a lot in terms of papers and tools on the topic of pruning. For TensorFlow, you can have a look at this more  [comprehensive_guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide).\n",
    "\n",
    "<img src=\"images/after_compression.png\" style=\"width:800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train sparse\n",
    "\n",
    "This time we'll use the Tensorflow model optimization sparsity to train a sparse model (forcing many weights to '0'). In this instance, the target sparsity is 75%.\n",
    "\n",
    "In particular we use the [prune_low_magnitude](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude) function. This function wraps a `tf.keras` model or layer with pruning functionality which sparsifies the layer's weights during training. For example, using this with 75% sparsity will ensure that 75% of the layer's weights are zero.\n",
    "\n",
    "Among the parameters for the `prune_low_magnitude` function, we specify `PruningSchedule` object to control the pruning rate throughout training.\n",
    "- In our case, we are using `ConstantSparsity`, which means that we are using constant sparsity (%) throughout training.\n",
    "- You can also use `PolynomialDecay` based sparsity, in tha case more or fewer sparsity can be used with increasing or decreasing speed, as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "We'll use the same settings as the model for part 1: Adam optimizer with categorical crossentropy loss.\n",
    "The callbacks will decay the learning rate and save the model into a directory 'model_2'\n",
    "The model isn't very complex, so this should just take a few minutes even on the CPU.\n",
    "If you've restarted the notebook kernel after training once, set `train = False` to load the trained model rather than training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(stop_patience = 1000,\n",
    "                              lr_factor = 0.5,\n",
    "                              lr_patience = 10,\n",
    "                              lr_epsilon = 0.000001,\n",
    "                              lr_cooldown = 2,\n",
    "                              lr_minimum = 0.0000001,\n",
    "                              outputDir = 'model_2')\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(X_train_val, y_train_val, batch_size=1024,\n",
    "              epochs=30, validation_split=0.25, shuffle=True,\n",
    "              callbacks = callbacks.callbacks)\n",
    "    model.save('model_2/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    model = load_model('model_2/KERAS_check_best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sparsity\n",
    "Make a quick check that the model was indeed trained sparse. We'll just make a histogram of the weights of the 1st layer, and hopefully observe a large peak in the bin containing '0'. Note logarithmic y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.layers[0].weights[0].numpy()\n",
    "h, b = np.histogram(w, bins=100)\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.bar(b[:-1], h, width=b[1]-b[0])\n",
    "plt.semilogy()\n",
    "print('% of zeros = {}'.format(np.sum(w==0)/np.size(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance\n",
    "\n",
    "How does this 75% sparse model compare against the unpruned model?\n",
    "\n",
    "Let's report the accuracy and make a ROC curve. The pruned model is shown with solid lines, the unpruned model from [Part 1](part1_getting_started.ipynb) is shown with dashed lines.\n",
    "\n",
    "**Make sure you've trained the model from Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
    "\n",
    "y_ref = model_ref.predict(X_test)\n",
    "y_prune = model.predict(X_test)\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(\"Unpruned Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Pruned   Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_prune, axis=1))))\n",
    "print('-----------------------------------')\n",
    "\n",
    "# Enable logarithmic scale on TPR and FPR axes \n",
    "logscale_tpr = False # Y axis\n",
    "logscale_fpr = False # X axis\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.plotMultiClassRoc(y_test, y_ref, classes, logscale_tpr=logscale_tpr, logscale_fpr=logscale_fpr)\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.plotMultiClassRoc(y_test, y_prune, classes, logscale_tpr=logscale_tpr, logscale_fpr=logscale_fpr, linestyle='--')\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], ls='-'),\n",
    "         Line2D([0], [0], ls='--')]\n",
    "from matplotlib.legend import Legend\n",
    "leg = Legend(ax, lines, labels=['unpruned', 'pruned'],\n",
    "            loc='center right', frameon=False)\n",
    "_ = ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the model to FPGA firmware with hls4ml\n",
    "Let's use the default configuration: `ap_fixed<16,6>` precision everywhere and `ReuseFactor=1`, so we can compare with the [Part 1](part1_getting_started.ipynb) model.\n",
    "\n",
    "We need to use the function [strip_pruning](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/strip_pruning) to change the layer types back to their originals: once a model has been pruned to required sparsity, this method can be used to restore the original model with the sparse weights.\n",
    "\n",
    "**This takes approx. 10 minutes on Columbia servers.**\n",
    "\n",
    "While the C-Synthesis is running, we can monitor the progress looking at the log file by opening a terminal from the notebook home, and executing:\n",
    "\n",
    "`tail -f model_2/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "import hls4ml\n",
    "\n",
    "# Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "model = strip_pruning(model)\n",
    "\n",
    "# Use the default configuration for hls4ml\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='model')\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='model_2/hls4ml_prj',\n",
    "                                                       #part='xczu7ev-ffvc1156-2-e') # ZCU106\n",
    "                                                       part='xczu3eg-sbva484-1-e') # Ultra96\n",
    "                                                       #part='xc7z020clg400-1') # Pynq-Z1\n",
    "                                                       #part='xc7z007sclg225-1') # MiniZed\n",
    "hls_model.compile()\n",
    "\n",
    "hls_results = hls_model.build(csim=False)\n",
    "\n",
    "# print('-----------------------------------')\n",
    "# #print(hls_results) # Print hashmap\n",
    "# print(\"Estimated Clock Period: {} ns\".format(hls_results['EstimatedClockPeriod']))\n",
    "# print(\"Best/Worst Latency:     {} / {}\".format(hls_results['BestLatency'], hls_results['WorstLatency']))\n",
    "# print(\"Interval Min/Max:       {} / {}\".format(hls_results['IntervalMin'], hls_results['IntervalMax']))\n",
    "# print(\"BRAM_18K:               {} (Aval. {})\".format(hls_results['BRAM_18K'], hls_results['AvailableBRAM_18K']))\n",
    "# print(\"DSP48E:                 {} (Aval. {})\".format(hls_results['DSP48E'], hls_results['AvailableDSP48E']))\n",
    "# print(\"FF:                     {} (Aval. {})\".format(hls_results['FF'], hls_results['AvailableFF']))\n",
    "# print(\"LUT:                    {} (Aval. {})\".format(hls_results['LUT'], hls_results['AvailableLUT']))\n",
    "# print(\"URAM:                   {} (Aval. {})\".format(hls_results['URAM'], hls_results['AvailableURAM']))\n",
    "# print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the reports\n",
    "Print out the reports generated by Vivado HLS. Pay attention to the Utilization Estimates' section in particular this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_2/hls4ml_prj/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION: you need to have trained and synthesized the model from Part 1**\n",
    "\n",
    "Print the report for the model trained in [Part 1](part1_getting_started.ipynb).\n",
    "\n",
    "- Remember these two models have the same architecture, but the model in this notebook was trained using the sparsity API from TensorFlow Model Optimization.\n",
    "- Notice how the resource usage had dramatically reduced (particularly the DSPs). When Vivado HLS notices an operation like `y = 0 * x` it can avoid placing a DSP for that operation. The impact of this is biggest when `ReuseFactor = 1`, but still applies at higher reuse as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, remember how we are learning how to reduce the hardware-resource usage without affecting (too much) the model accuracy.\n",
    "\n",
    "<img src=\"images/boards.png\" style=\"width:640pt;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

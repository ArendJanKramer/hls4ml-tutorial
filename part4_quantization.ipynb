{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "The current set of notebooks are under constant development.\n",
    "\n",
    "## Update Tutorial Repository\n",
    "\n",
    "If you have previously cloned the tutorial repository, you may need to get the latest versions of the notebooks.\n",
    "\n",
    "First check the status of your repository:\n",
    "```\n",
    "cd hls4ml-tutorial\n",
    "git status \n",
    "```\n",
    "\n",
    "You may have some _modified_ notebooks. For example:\n",
    "\n",
    "```\n",
    "# On branch csee-e6868-spring2022\n",
    "# Changes not staged for commit:\n",
    "#   (use \"git add <file>...\" to update what will be committed)\n",
    "#   (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
    "#\n",
    "#\tmodified:   part1_getting_started.ipynb\n",
    "#\tmodified:   part2_advanced_config.ipynb\n",
    "#\n",
    "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
    "```\n",
    "\n",
    "You can make a copy of those modified notebooks if you had significat changes, otherwise the easiest thing to do is to discard those changes.\n",
    "\n",
    "**ATTENTION** You will loose your local changes!\n",
    "\n",
    "```\n",
    "git checkout *.ipynb\n",
    "```\n",
    "\n",
    "At this point, you can update you copy of the repository:\n",
    "```\n",
    "git pull\n",
    "```\n",
    "\n",
    "# Part 4: Quantization\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "As we did in the previous notebooks, let's import the libraries, call the magic functions, and setup the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/xilinx/Vivado/2019.1/bin:' + os.environ['PATH']\n",
    "def is_tool(name):\n",
    "    from distutils.spawn import find_executable\n",
    "    return find_executable(name) is not None\n",
    "\n",
    "print('-----------------------------------')\n",
    "if not is_tool('vivado_hls'):\n",
    "    print('Xilinx Vivado HLS is NOT in the PATH')\n",
    "else:\n",
    "    print('Xilinx Vivado HLS is in the PATH')\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the jet tagging dataset from Open ML\n",
    "\n",
    "The [jet tagging dataset](https://www.openml.org/d/42468) is publicly available on [OpenML](https://www.openml.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load('X_train_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train_val = np.load('y_train_val.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "classes = np.load('classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a model\n",
    "This time we're going to use QKeras layers.\n",
    "\n",
    "QKeras is **Quantized Keras** for deep heterogeneous quantization of ML models.\n",
    "\n",
    "https://github.com/google/qkeras\n",
    "\n",
    "It is maintained by Google and we recently added support for QKeras model to hls4ml.\n",
    "\n",
    "See also [our paper](https://arxiv.org/abs/2006.10159) (co-authored with Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from callbacks import all_callbacks\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# Note the Q-layers from the QKeras package\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple _multi-layer perceptron (MLP)_ model. An MLP consists of at least three dense layers of nodes alternating with activation functions.\n",
    "\n",
    "**EXACTLY AS BEFORE**\n",
    "\n",
    "- We use the [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) that is essentially a stack of layers, where each layer has exactly one input tensor and one output tensor. See also the [Functional API](https://www.tensorflow.org/guide/keras/functional).\n",
    "- We use _3_ hidden layers with _64_, then _32_, then _32_ neurons. See a plot of the model in the next few cell.\n",
    "- Each layer will use ReLU activation.\n",
    "- Add an output layer with _5_ neurons (one for each class), then finish with Softmax activation.\n",
    "- [Initializers](https://keras.io/api/layers/initializers) define the way to set the initial random weights of Keras layers. In this case, we choose [LecunUniform](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/LecunUniform).\n",
    "- [Regularizers](https://keras.io/api/layers/regularizers) help to get models that generalize to new, unseen data (see the [overfitting problem](https://en.wikipedia.org/wiki/Overfitting)); the regularizes allow you to apply penalties on layer parameters or layer activity during optimization. These penalties are summed into the loss function that the network optimizes. In this case, we choose [L1 regularization](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1) that producer _sparse models_, i.e. model where unnecessary features are set to zero, thus do not contribute to the model predictive power.\n",
    "\n",
    "**WHAT'S NEW**\n",
    "\n",
    "- We're using `QDense` layer instead of `Dense`, and `QActivation` instead of `Activation`.\n",
    "- We're also specifying `kernel_quantizer = quantized_bits(6,0,alpha=1)`. This will use 6-bit (of which 0 are integer and the sign bit is implicit) for the weights.\n",
    "- We also use the same quantization for the biases, and `bias_quantizer=quantized_bits(6,0,alpha=1)` for 6-bit ReLU activations.\n",
    "- `alpha=1` is scaling factor that we can ignore for the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(QDense(64, input_shape=(16,), name='fc1',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(QDense(32, name='fc2',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(QDense(32, name='fc3',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
    "model.add(QDense(5, name='output',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train sparse\n",
    "\n",
    "Let's train with model sparsity again, since QKeras layers are prunable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll use the same settings as the model for [Part 1](part1_getting_started.ipynb): Adam optimizer with categorical crossentropy loss.\n",
    "\n",
    "The callbacks will decay the learning rate and save the model into a directory `model_3`. The model isn't very complex, so this should just take a few minutes even on the CPU. If you've restarted the notebook kernel after training once, set `train = False` to load the trained model rather than training again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(stop_patience = 1000,\n",
    "                              lr_factor = 0.5,\n",
    "                              lr_patience = 10,\n",
    "                              lr_epsilon = 0.000001,\n",
    "                              lr_cooldown = 2,\n",
    "                              lr_minimum = 0.0000001,\n",
    "                              outputDir = 'model_3')\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(X_train_val, y_train_val, batch_size=1024,\n",
    "              epochs=30, validation_split=0.25, shuffle=True,\n",
    "              callbacks = callbacks.callbacks)\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_3/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('model_3/KERAS_check_best_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance\n",
    "\n",
    "How does this model which was trained using 6-bits, and 75% sparsity model compare against the original model in [Part 1](part1_getting_started.ipynb)? \n",
    "\n",
    "Let's report the accuracy and make a ROC curve. The **quantized & pruned model** is shown with **solid lines**, the **unpruned model** from [Part 1](part1_getting_started.ipynb) is shown with **dashed lines**.\n",
    "\n",
    "We should also check that hls4ml _can respect_ the choice to use 6-bits throughout the model, and match the accuracy. We'll generate a **hls4ml model** configuration from this Quantized model, and plot its performance as the **dotted line**.\n",
    "\n",
    "When the generated configuration is printed out, you'll notice that it uses 7 bits for the type, but we specified 6 !? That's just because QKeras doesn't count the sign-bit when we specify the number of bits, so the type that actually gets used needs 1 more bit.\n",
    "\n",
    "We also use the `OutputRoundingSaturationMode` optimizer pass of `hls4ml` to set the Activation layers\n",
    "- to _round_, rather than truncate, the cast. This is important for getting good model accuracy when using small bit precision activations.\n",
    "- to _saturate_, rather than wrap around in case of overflows.\n",
    "\n",
    "<img src=\"images/saturation_overflow.png\" style=\"width:1024px;\" />\n",
    "\n",
    "Finally we'll set a different data type for the tables used in the Softmax, just for a bit of extra performance.\n",
    "\n",
    "\n",
    "**Make sure you've trained the model from part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(model,\n",
    "                                                       hls_config=config,\n",
    "                                                       output_dir='model_3/hls4ml_prj',\n",
    "                                                       #fpga_part='xczu7ev-ffvc1156-2-e') # ZCU106\n",
    "                                                       fpga_part='xczu3eg-sbva484-1-e') # Ultra96\n",
    "                                                       #fpga_part='xc7z020clg400-1') # Pynq-Z1\n",
    "                                                       #fpga_part='xc7z007sclg225-1') # MiniZed\n",
    "hls_model.compile()\n",
    "\n",
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_ref = load_model('model_1/KERAS_check_best_model.h5')\n",
    "y_ref = model_ref.predict(X_test)\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(\"Baseline accuracy:          {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_ref, axis=1))))\n",
    "print(\"Pruned, quantized accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"hls4ml accuracy:            {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n",
    "print('-----------------------------------')\n",
    "\n",
    "\n",
    "# Enable logarithmic scale on TPR and FPR axes \n",
    "logscale_tpr = False # Y axis\n",
    "logscale_fpr = False # X axis\n",
    "fig, ax = plt.subplots(figsize=(9, 9))\n",
    "_ = plotting.plotMultiClassRoc(y_test, y_ref, classes, logscale_tpr=logscale_tpr, logscale_fpr=logscale_fpr, linestyle='-')\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.plotMultiClassRoc(y_test, y_qkeras, classes, logscale_tpr=logscale_tpr, logscale_fpr=logscale_fpr, linestyle='--')\n",
    "plt.gca().set_prop_cycle(None) # reset the colors\n",
    "_ = plotting.plotMultiClassRoc(y_test, y_hls, classes, logscale_tpr=logscale_tpr, logscale_fpr=logscale_fpr, linestyle=':')\n",
    "\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "lines = [Line2D([0], [0], ls='-'),\n",
    "         Line2D([0], [0], ls='--'),\n",
    "         Line2D([0], [0], ls=':')]\n",
    "from matplotlib.legend import Legend\n",
    "leg = Legend(ax, lines, labels=['baseline', 'pruned, quantized', 'hls4ml'],\n",
    "            loc='lower right', frameon=False)\n",
    "_ = ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize\n",
    "Now let's synthesize this quantized, pruned model.\n",
    "\n",
    "**The synthesis will take a while**\n",
    "\n",
    "While the C-Synthesis is running, we can monitor the progress looking at the log file by opening a terminal from the notebook home, and executing:\n",
    "\n",
    "`tail -f model_3/hls4ml_prj/vivado_hls.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_results = hls_model.build(csim=False)\n",
    "\n",
    "# print('-----------------------------------')\n",
    "# #print(hls_results) # Print hashmap\n",
    "# print(\"Estimated Clock Period: {} ns\".format(hls_results['EstimatedClockPeriod']))\n",
    "# print(\"Best/Worst Latency:     {} / {}\".format(hls_results['BestLatency'], hls_results['WorstLatency']))\n",
    "# print(\"Interval Min/Max:       {} / {}\".format(hls_results['IntervalMin'], hls_results['IntervalMax']))\n",
    "# print(\"BRAM_18K:               {} (Aval. {})\".format(hls_results['BRAM_18K'], hls_results['AvailableBRAM_18K']))\n",
    "# print(\"DSP48E:                 {} (Aval. {})\".format(hls_results['DSP48E'], hls_results['AvailableDSP48E']))\n",
    "# print(\"FF:                     {} (Aval. {})\".format(hls_results['FF'], hls_results['AvailableFF']))\n",
    "# print(\"LUT:                    {} (Aval. {})\".format(hls_results['LUT'], hls_results['AvailableLUT']))\n",
    "# print(\"URAM:                   {} (Aval. {})\".format(hls_results['URAM'], hls_results['AvailableURAM']))\n",
    "# print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the reports\n",
    "Print out the reports generated by Vivado HLS. Pay attention to the Utilization Estimates' section in particular this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_3/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report for the model trained in [Part 1](part1_getting_started.ipynb). Now, compared to the model from [Part 1](part1_getting_started.ipynb), this model has been trained with low-precision quantization, and 75% pruning. You should be able to see that we have saved a lot of resource compared to where we started in part 1. At the same time, referring to the ROC curve above, the model performance is pretty much identical even with this drastic compression!\n",
    "\n",
    "**Note you need to have trained and synthesized the model from Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_1/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the report for the model trained in part 3. Both these models were trained with 75% sparsity, but the new model uses 6-bit precision as well. You can see how Vivado HLS has moved multiplication operations from DSPs into LUTs, reducing the \"critical\" resource usage.\n",
    "\n",
    "**Note you need to have trained and synthesized the model from part 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('model_2/hls4ml_prj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.B. Vivado HLS Overstimates Resources \n",
    "\n",
    "Note as well that the Vivado HLS resource estimates tend to _overestimate_ LUTs, while generally estimating the DSPs correctly. Running the subsequent stages of FPGA compilation reveals the more realistic resource usage, You can run the next step, 'logic synthesis' with `hls_model.build(synth=True, vsynth=True)`, but we skipped it in this tutorial in the interest of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Conclude\n",
    "\n",
    "After these lectures you have\n",
    "- gained some hands on experience with hls4ml: you translated a NN to FPGA firmware, run simulation and synthesis;\n",
    "- tuned NN inference performance withe precision and reuse factor;\n",
    "- learned how to simply prune a NN and see the impact on resources;\n",
    "- trained a model with small numbers of bits using QKeras and see the impact on resources.\n",
    "\n",
    "This is just an introduction to the tool and we [welcome](https://fastmachinelearning.org) people joining the open-source project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
